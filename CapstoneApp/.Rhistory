library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
modelfit=svm(CompressiveStrength~.,data=training)
pred1=predict(modelfit,testing)
confusionMatrix(pred1,testing$CompressiveStrength)
sqrt(sum((pred1-testing$CompressiveStrength)^2))
library(e1071)
library(caret)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
modelfit=svm(CompressiveStrength~.,data=training)
pred1=predict(modelfit,testing)
sqrt(sum((pred1-testing$CompressiveStrength)^2))
sqrt(sum((pred1-testing$CompressiveStrength)^2))/length(pred1)
rmse(pred1,testing$CompressiveStrength)
sqrt(mean((pred1-testing$CompressiveStrength)^2))
confusionMatrix(testing$CompressiveStrength,pred1)
library(lubridate) # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
install.packages("lubridate")
library(lubridate) # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
?bats()
?bats
??bats
install.packages("BAT")
?bats
library(lubridate) # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
modeflfit=bats(testing)
library(BATS)
library(BAT)
modeflfit=bats(testing)
install.packages("forecast")
modeflfit=bats(testing)
library(forecast)
modeflfit=bats(testing)
modeflfit= bats(tstrain)
pred=predict(modelfit,tstest)
tstest = ts(training$visitsTumblr)
pred=predict(modelfit,tstest)
library(lubridate) # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
tstest = ts(training$visitsTumblr)
library(forecast)
modeflfit= bats(tstrain)
pred=predict(modelfit,tstest)
pred=forecast(modelfit,tstest)
?forecast
library(lubridate) # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
tstest = ts(training$visitsTumblr)
library(forecast)
modeflfittrain= bats(tstrain)
modeflfittest= bats(tstest)
pred=forecast(modeflfittest)
accuracy(pred,tstest)
pred1
fcast=forecast(modeflfittest)
accuracy(fcast,tstest)
confusionMatrix(fcast,tstest)
modeflfittrain= bats(tstrain)
modeflfittest= bats(tstest)
fcast=forecast(modeflfittest)
confusionMatrix(fcast,tstest)
?confusionMatrix
?accuracy
accuracy(fcast,tstest)
tstest
fcast
accuracy(fcast,tstest)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
modelfit1=train(diagnosis~.,method="rf",data=training)
modelfit2=train(diagnosis~.,method="gbm",data=training,verbose=FALSE)
modelfit3=train(diagnosis~.,method="lda",data=training)
pred1=predict(modelfit1,testing[,-1])
pred2=predict(modelfit2,testing[,-1])
pred3=predict(modelfit3,testing[,-1])
predDF=data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
combmodfit=train(diagnosis~.,method="rf",data=predDF)
combpred=predict(combmodfit,predDF)
confusionMatrix(combpred,testing$diagnosis)
confusionMatrix(pred1,testing$diagnosis)
confusionMatrix(pred2,testing$diagnosis)
confusionMatrix(pred3,testing$diagnosis)
library(lubridate) # For year() function below
dat = read.csv("~/Desktop/gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
tstest = ts(training$visitsTumblr)
library(forecast)
modeflfittrain= bats(tstrain)
modeflfittest= bats(tstest)
fcast=forecast(modeflfittest)
length(tstrain)
length(tstest)
class(fcast)
dim(fcast)
fcast
fcast=forecast(modeflfittrain)
fcast
fcast$fitted
confusionMatrix(fcast$fitted,tstest)
tstest
modeflfittest= bats(tstest)
?forecats
?forecast
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
#modelfit=train(CompressiveStrength~.,method="lasso",data=training)
modelfit=enet(as.matrix(training[,-9]),as.matrix(training[,9]),lambda=0)
png("plot1.png",width = 480, height = 480, units = "px")
plot(modelfit)
dev.off()
set.seed(3523)
library(AppliedPredictiveModeling)
library(enet)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
#modelfit=train(CompressiveStrength~.,method="lasso",data=training)
modelfit=enet(as.matrix(training[,-9]),as.matrix(training[,9]),lambda=0)
png("plot1.png",width = 480, height = 480, units = "px")
plot(modelfit)
dev.off()
?enet
??enet
library(elasticnet)
modelfit=enet(as.matrix(training[,-9]),as.matrix(training[,9]),lambda=0)
png("plot1.png",width = 480, height = 480, units = "px")
plot(modelfit)
dev.off()
plot.enet(modelfit)
?plot.enet
?plot.enet
library(AppliedPredictiveModeling)
library(elasticnet)
library(enet)
library(caret)
?plot.enet
set.seed(3523)
library(AppliedPredictiveModeling)
library(elasticnet)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
#modelfit=train(CompressiveStrength~.,method="lasso",data=training)
modelfit=enet(as.matrix(training[,-9]),as.matrix(training[,9]),lambda=0)
summary(modelfit)
modelfit
enet
?enet
set.seed(3523)
library(AppliedPredictiveModeling)
library(elasticnet)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
#modelfit=train(CompressiveStrength~.,method="lasso",data=training)
modelfit=enet(as.matrix(training[,-9]),as.matrix(training[,9]),lambda=0)
png("plot1.png",width = 480, height = 480, units = "px")
plot(modelfit)
dev.off()
names(training)
install.packages("LATEX")
library(UsingR)
mod=lm(parent~child,data="Galton")
mod=lm(parent~child,data=Galton)
summary(mod)
options(warn=-1)
suppressPackageStartupMessages(library(ggplot2));suppressPackageStartupMessages(library(UsingR))
p=qplot(parent,child,data=Galton,geom=c("point", "smooth"),abline=lm(parent~child,data=Galton))
print(p)
options(warn=-1)
suppressPackageStartupMessages(library(ggplot2));suppressPackageStartupMessages(library(UsingR))
p=qplot(parent,child,data=Galton,geom=c("point", "smooth"))
p=p+abline(lm(parent~child,data=Galton),lwd=2)
print(p)
options(warn=-1)
suppressPackageStartupMessages(library(ggplot2));suppressPackageStartupMessages(library(UsingR))
p=plot(parent,child,data=Galton,geom=c("point", "smooth"))
p=p+abline(lm(parent~child,data=Galton),lwd=2)
print(p)
library(ggplot2)
options(warn=-1)
suppressPackageStartupMessages(library(ggplot2));suppressPackageStartupMessages(library(UsingR))
p=plot(parent,child,data=Galton,geom=c("point", "smooth"))
p=p+abline(lm(parent~child,data=Galton),lwd=2)
print(p)
suppressPackageStartupMessages(library(ggplot2));suppressPackageStartupMessages(library(UsingR))
p=plot(parent,child,data=Galton)
p=p+abline(lm(parent~child,data=Galton),lwd=2)
print(p)
options(warn=-1)
suppressPackageStartupMessages(library(ggplot2));suppressPackageStartupMessages(library(UsingR))
data(Galton)
p=plot(parent,child,data=Galton)
p=p+abline(lm(parent~child,data=Galton),lwd=2)
print(p)
data(Galton)
head(Galton)
options(warn=-1)
suppressPackageStartupMessages(library(ggplot2));suppressPackageStartupMessages(library(UsingR))
data(Galton)
p=plot(Galton$parent,Galton$child,xlab="Parent's height",ylab="Child's Height",bg="lightblue",col="Black",pch=21)
p=p+abline(lm(parent~child,data=Galton),lwd=2)
print(p)
options(warn=-1)
suppressPackageStartupMessages(library(ggplot2));suppressPackageStartupMessages(library(UsingR))
data(Galton)
p=plot(Galton$parent,Galton$child,xlab="Parent's height",ylab="Child's Height",bg="lightblue",col="Black",pch=10)
p=p+abline(lm(parent~child,data=Galton),lwd=2)
print(p)
?qplot
?ggplot
p=qplot(parent,child,data=Galton)
print(p)
p=p+abline(lm(parent~child,data=Galton),lwd=2,col="red")
print(p)
p=p+abline(lm(parent~child,data=Galton),lwd=2)
dev.off()
p=qplot(parent,child,data=Galton)
p=p+abline(lm(parent~child,data=Galton),lwd=2,col="red")
print(p)
p=qplot(parent,child,data=Galton,geom=c("smooth"))
dev.off()
p=qplot(parent,child,data=Galton,geom=c("smooth"))
print(p)
dev.off()
p=plot(Galton$parent,Galton$child,xlab="Parent's height",ylab="Child's Height",bg="lightblue",col="Black",cex=1.1,pch=21,frame=FALSE)
p=p+abline(lm(parent~child,data=Galton),lwd=2,col="red")
suppressFunctionOutput()
?suppress
install.packages("tm")
corpus<-Corpus(DirSource("/Users/malaydas/Documents/Data Science/Capstone Project/final/en_US"), readerControl = list(reader=readPlain))
library(tm)
library(stringi)
corpus<-Corpus(DirSource("/Users/malaydas/Documents/Data Science/Capstone Project/final/en_US"), readerControl = list(reader=readPlain))
summary(corpus)
corpus[[1][1]]
corpus[1][1]
corpus[1]
corpus[[1]]
corpus[[1][1]]
corpus[[1]][1]
corpus[[1]][1][1]
corpus[[2]][1]
corpus[[2]][1:3]
corpus[[2]]
corpus[[2]][[1][1]]
corpus[[2]][[1]
]
```{r,echo=FALSE,results='hide'}
## Removing Punctuation
doc <- tm_map(doc,removePunctuation)
gc()
## Removing White Space
doc <- tm_map(doc,stripWhitespace)
gc()
## Removing stopwords
doc <- tm_map(doc, removeWords, stopwords("english"))
gc()
## Removing common word endings (e.g., “ing”, “es”, “s”)
doc <- tm_map(doc, stemDocument)
gc()
## Removing Numbers
doc <- tm_map(doc,removeNumbers)
gc()
```
load("/Users/malaydas/Downloads/transitionMatrix.RData")
View(transitionMatrix)
install.packages("markovchain")
?markovchain
library(markovchain)
?markovchain
library(markovchain)
?markovchain
load("/Users/malaydas/Downloads/transitionMatrix.RData")
View(transitionMatrix)
?unlist
?states
suppressPackageStartupMessages(library(tm));suppressPackageStartupMessages(library(RWeka));
suppressPackageStartupMessages(library(stringi));suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(tidyr));suppressPackageStartupMessages(library(R.utils))
suppressPackageStartupMessages(library(plyr));suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(markovchain))
?states
runif(1)
?paste0
commonIdx = numeric()
load("/Users/malaydas/Downloads/transitionMatrix (2).RData")
View(transitionMatrix)
?rbinom
?rbinom()
OutputData="/Users/malaydas/Documents/Data Science/Capstone Project/data/OutputData/"
TransitionMatrixPath = file.path(OutputData,"Initial_Trigram_TransitionMatrix.RData")
load(TransitionMatrixPath)
sum(transitionMatrix[1,0])
sum(transitionMatrix[1,])
sum(transitionMatrix[2,])
sum(transitionMatrix[5,])
sum(transitionMatrix[100,])
sum(transitionMatrix[900],])
sum(transitionMatrix[900,])
View(transitionMatrix)
load(file=file.path(OutputData,"Unigram.RData"))
tail(Unigram)
Unigram[which(Unigram$count)==60]
Unigram[which(Unigram$count==60)]
Unigram[which(Unigram$count==60),]
?subset
Unigram=subset(Unigram,count>49)
tail(Unigram)
load(file.path(OutputData,"Unigram.RData"))
Unigram=subset(Unigram,count>9)
tail(Unigram)
load(file.path(OutputData,"Unigram.RData"))
Unigram=subset(Unigram,count>19)
tail(Unigram)
load(file.path(OutputData,"Unigram.RData"))
Unigram=subset(Unigram,count>29)
tail(Unigram)
load(file.path(OutputData,"Trigram.RData"))
tail(Trigram)
head(Trigram)
grep("'",Trigram$word)
grep("for the",Trigram$word)
head(grep("for the",Trigram$word))
Trigram[head(grep("for the",Trigram$word)),]
tail(subset(Trigram,Trigram$count>5))
CurWords = unlist(str_split(names(MatchTrigram[1])," "))
?str_split
str_split(Trigram$words[1])
library(stringr)
str_split(Trigram$words[1])
library(stringi)
str_split(Trigram$words[1])
x<- "I can't do this anymore.They should've been carefull"
gsub("[[:punct:]]"," ",tolower(x))
curPhrase<- "I am not well"
curPhrase[1]
library(MarkovChain)
library(markovchain)
?conditionalDistribution
InputPhrase =    unlist(str_split(as.character(CurPhrase)," "))
InputPhrase =    unlist(str_split(as.character(curPhrase)," "))
library(stringi)
InputPhrase =    unlist(str_split(as.character(curPhrase)," "))
library(stringr)
InputPhrase =    unlist(str_split(as.character(curPhrase)," "))
InputPhrase
length(InputPhrase)
class(InputPhrase)
class(curPhrase)
- Bullet 1
---
title       : Predicting Children's Height
- Bullet 1
- Bullet 2
date()
? date
format(date(),"%a %b %c")
?format
format(date(),"%d %m %y")
format(date())
format(date(),"dd-mm-yyyy")
format(date(),format="%B %d %Y")
format(today,format="%B %d %Y")
format(sys.date(),format="%B %d %Y")
format(Sys.Date(),format="%B %d %Y")
shiny::runApp('Documents/Data Science/Capstone Project/ProgramingAssignment/Capstone/App')
?trim
shiny::runApp('Documents/Data Science/Capstone Project/ProgramingAssignment/Capstone/App')
shiny::runApp('Documents/Data Science/Capstone Project/ProgramingAssignment/Capstone/App')
shiny::runApp('Documents/Data Science/Capstone Project/ProgramingAssignment/Capstone/App')
shiny::runApp('Documents/Data Science/Capstone Project/ProgramingAssignment/Capstone/App')
shiny::runApp('Documents/Data Science/Capstone Project/ProgramingAssignment/Capstone/App')
shiny::runApp('Documents/Data Science/Capstone Project/ProgramingAssignment/Capstone/App')
</style>
shiny::runApp('Documents/Data Science/Capstone Project/ProgramingAssignment/Capstone/App')
## Setting up the processing directory
ProgramDir="/Users/malaydas/Documents/Data Science/Capstone Project/ProgramingAssignment/Capstone/";setwd(ProgramDir)
AppDir="/Users/malaydas/Documents/Data Science/Capstone Project/ProgramingAssignment/Capstone/App/"
InputData="/Users/malaydas/Documents/Data Science/Capstone Project/data/InputData/en_US/"
OutputData="/Users/malaydas/Documents/Data Science/Capstone Project/data/OutputData/"
SampleData="/Users/malaydas/Documents/Data Science/Capstone Project/data/SampleData/"
TrainingData="/Users/malaydas/Documents/Data Science/Capstone Project/data/TrainingData/"
TestData="/Users/malaydas/Documents/Data Science/Capstone Project/data/TestData/"
ValidationData="/Users/malaydas/Documents/Data Science/Capstone Project/data/ValidationData/"
## Utility program inclusion
source("./Capstone_Utilities.R")
source("./EvaluateModel.R")
Corpus(VectorSource(MergedVector),readerControl = list(reader=readPlain))
x="i'll never go, and"
x<-Corpus(VectorSource(x),readerControl = list(reader=readPlain))
x<-PreprocessCorpus(x,blackList)
setwd(InputData)
## Sourcing blackListed/profane word
blackList <- readLines("./en_profanity.txt",encoding='UTF-8',skipNul=TRUE)
x<-PreprocessCorpus(x,blackList)
x
x[[1]]
x[[1]][1]
x<-x[[1]][1]
x
shiny::runApp('~/Documents/Data Science/Capstone Project/ProgramingAssignment/Capstone/App')
x="having days off,"
removePunct (x)
x="having days off."
removePunct (x)
shiny::runApp('~/Documents/Data Science/Capstone Project/ProgramingAssignment/Capstone/App')
x="#ppv each #weightwatchers"
removePunct (x)
shiny::runApp('~/Documents/Data Science/Capstone Project/ProgramingAssignment/Capstone/App')
shiny::runApp('~/Documents/Data Science/Capstone Project/ProgramingAssignment/Capstone/App')
shiny::runApp('~/Documents/Data Science/Capstone Project/ProgramingAssignment/Capstone/App')
shiny::runApp('~/Documents/Data Science/Capstone Project/ProgramingAssignment/Capstone/App')
x="#ppv each #weightwatchers"
ToMatch = removePunct(x)
ToMatch = unlist(str_split(as.character(tolower(str_trim(ToMatch)))," "))
temp = list()
# Check with Stupid Back off
WordPredictedS = PredictNextWordUsingStupidBackoff(ToMatch,Quadrigram,Trigram,Bigram)
# Check with Markov Matrix
WordPredictedM = PredictNextWordUsingMarkov(ToMatch,MarkovMatrix)
load(file="./MarkovObjects.RData")
load(file="./Quadrigram.RData")
load(file="./Trigram.RData")
load(file="./Bigram.RData")
rm(TransitionMatrix)
## Setting up the processing directory
ProgramDir="/Users/malaydas/Documents/Data Science/Capstone Project/ProgramingAssignment/Capstone/";setwd(ProgramDir)
AppDir="/Users/malaydas/Documents/Data Science/Capstone Project/ProgramingAssignment/Capstone/App/"
InputData="/Users/malaydas/Documents/Data Science/Capstone Project/data/InputData/en_US/"
OutputData="/Users/malaydas/Documents/Data Science/Capstone Project/data/OutputData/"
SampleData="/Users/malaydas/Documents/Data Science/Capstone Project/data/SampleData/"
TrainingData="/Users/malaydas/Documents/Data Science/Capstone Project/data/TrainingData/"
TestData="/Users/malaydas/Documents/Data Science/Capstone Project/data/TestData/"
ValidationData="/Users/malaydas/Documents/Data Science/Capstone Project/data/ValidationData/"
setwd(AppDir)
load(file="./MarkovObjects.RData")
load(file="./Quadrigram.RData")
load(file="./Trigram.RData")
load(file="./Bigram.RData")
rm(TransitionMatrix)
ToMatch = unlist(str_split(as.character(tolower(str_trim(ToMatch)))," "))
temp = list()
# Check with Stupid Back off
WordPredictedS = PredictNextWordUsingStupidBackoff(ToMatch,Quadrigram,Trigram,Bigram)
# Check with Markov Matrix
WordPredictedM = PredictNextWordUsingMarkov(ToMatch,MarkovMatrix)
WordPredictedS$ExactWordPredicted
WordPredictedM$ExactWordPredicted
is.null(WordPredictedS$ExactWordPredicted)
as.vector(colnames(PossibleNextWordsCP))[2:5]
PossibleNextWordsCP =t(as.matrix(WordPredictedM$PossibleWordConditionalProbability))
as.vector(colnames(PossibleNextWordsCP))[2:5]
WordPredictedS$ExactWordPredicted != WordPredictedM$ExactWordPredicted
shiny::runApp()
shiny::runApp()
shiny::runApp()
WordPredictedM$ExactWordPredicted
WordPredictedS$ExactWordPredicted != WordPredictedM$ExactWordPredicted && !is.null(WordPredictedS$ExactWordPredicted)
WordPredictedS$ExactWordPredicted != WordPredictedM$ExactWordPredicted || !is.null(WordPredictedS$ExactWordPredicted)
shiny::runApp()
x
shiny::runApp()
shiny::runApp()
